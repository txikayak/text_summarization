{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aim: An extractive text summarization for PubMed article abstracts\n",
    "\n",
    "Citation: The core of this extractive text summarization is based on Ng Wai Foong's post:\n",
    "Extractive Text Summarization Using spaCy in Python \n",
    "https://medium.com/better-programming/extractive-text-summarization-using-spacy-in-python-88ab96d1fd97\n",
    "\n",
    "Modifications from my side:\n",
    "1) Tracking case sensitive terms and replace them in the summary\n",
    "2) Replacing \"We\" to \"The authors\" and \"Our\" to \"The authors'\"\n",
    "3) Normalized \"Conclusion:\" etc into \"In conclusion\" and give additional weights to these sentences\n",
    "4) Additional text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --user -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip HTLM tags\n",
    "# Citation: https://stackoverflow.com/a/925630\n",
    "from io import StringIO\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.text = StringIO()\n",
    "    def handle_data(self, d):\n",
    "        self.text.write(d)\n",
    "    def get_data(self):\n",
    "        return self.text.getvalue()\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up line breaks, HTML tags, extra spaces, and first tone plural\n",
    "def clean_text(text):\n",
    "    \n",
    "    # Citation: lrnq @ github for this space normalization step\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Remove line breaks\n",
    "    # Citation: https://stackoverflow.com/a/16566356\n",
    "    text = text.replace('\\n', ' ').replace('\\r', '')\n",
    "    \n",
    "    # Strip HTML tags\n",
    "    text = strip_tags(text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    # Citation: https://stackoverflow.com/a/1546244\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    \n",
    "    # Remove spaces before punctuation\n",
    "    # Citation: https://stackoverflow.com/a/18878970\n",
    "    text = re.sub(r'\\s([?,.!\"):;](?:\\s|$))', r'\\1', text)\n",
    "    \n",
    "    # Change \"We\" and \"Our\" etc\n",
    "    text = text.replace('We ', 'The authors ')\n",
    "    text = text.replace(' we ', ' the authors ')\n",
    "    text = text.replace('Our ', 'The authors\\' ')\n",
    "    text = text.replace(' our ', ' the authors\\' ')\n",
    "    \n",
    "    # Change \"conlusion\"\n",
    "    text = text.replace('CONCLUSION:', 'In conclusion, ')\n",
    "    text = text.replace('CONCLUSION ', 'In conclusion, ')\n",
    "    text = text.replace('CONCLUSIONS:', 'In conclusion, ')\n",
    "    text = text.replace('CONCLUSIONS ', 'In conclusion, ')\n",
    "    text = text.replace('Conclusion:', 'In conclusion, ')\n",
    "    text = text.replace('Conclusion ', 'In conclusion, ')\n",
    "    text = text.replace('Conclusions:', 'In conclusion, ')\n",
    "    text = text.replace('Conclusions ', 'In conclusion, ')\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track case sensitive tokens\n",
    "def track_case(text):\n",
    "    cased_words = []\n",
    "    ignore = [\"BACKGROUND\", \"PURPOSE\", \"METHODS\", \"RESULTS\", \"CONCLUSION\", \"DISCUSSION\", \"AIMS\", \"INTRODUCTION\", \"AND\", \"ANALYSIS\", \"DESIGN\", \"ETHICS\", \"DISSEMINATION\", \"TRIAL\", \"REGISTRATION\", \"NUMBER\", \"CONCLUSIONS\", \"AIMS/INTRODUCTION\", \"MATERIALS\", \"STUDY\", \"TYPE\", \"POPULATION\", \"FIELD\", \"STRENGTH/SEQUENCE\", \"STRENGTH\",  \"SEQUENCE\", \"ASSESSMENT\", \"STATISTICAL\", \"TESTS\", \"DATA\", \"LEVEL\", \"OF\", \"EVIDENCE\", \"OBJECTIVE\", \"MAIN\", \"KEY\", \"FINDINGS\", \"SIGNIFICANCE\", \"TRANSLATIONAL\", \"PERSPECTIVE\"]\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        token = token.text\n",
    "        if token != token.lower() and re.search('^[A-Z][a-z]+$', token) == None and len(token)>1 and token not in ignore:\n",
    "            cased_words.append(token)\n",
    "    \n",
    "    text = text.replace('(', ' ')\n",
    "    text = text.replace(')', ' ')\n",
    "    text = text.replace(':', ' ')\n",
    "    text = text.replace(',', ' ')\n",
    "    text = text.replace(';', ' ')\n",
    "    \n",
    "    # Since spaCy breaks terms with hypens, I go through the text again, with manual tokenization and identification of hypen tokens\n",
    "    for token in text.split():\n",
    "        if \"-\" in token and token != token.lower() and re.search('^[A-Z][a-z]+$', token) == None:\n",
    "            cased_words.append(token)\n",
    "    \n",
    "    return set(cased_words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change(text, word):\n",
    "    \n",
    "    # Citation: lrnq @ github solution\n",
    "    return ' '.join([x.replace(word.lower(), word) if x == word.lower()\n",
    "                    or x == \"(\" + word.lower() + \"),\" \n",
    "                     or x == \"(\" + word.lower() + \").\"\n",
    "                     or x == \"(\" + word.lower() + \");\"\n",
    "                     or x == \"(\" + word.lower() + \"):\"\n",
    "                     or x == \"(\" + word.lower() + \")\"\n",
    "                     or x == word.lower() + \",\"\n",
    "                     or x == word.lower() + \".\"\n",
    "                     or x == word.lower() + \";\"\n",
    "                     or x == word.lower() + \":\"\n",
    "                     else x for x in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_case(text, case_list):\n",
    "    for word in case_list:\n",
    "        text = change(text, word)\n",
    "    return text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_sentence(text, limit):\n",
    "    '''\n",
    "    Args:\n",
    "        text - the input text\n",
    "        limit - the number of sentences to return\n",
    "    '''\n",
    "    \n",
    "    text = clean_text(text)\n",
    "    cased = track_case(text)\n",
    "    \n",
    "    keyword = []\n",
    "    pos_tag = ['PROPN', 'ADJ', 'NOUN', 'VERB']\n",
    "    \n",
    "    # to lower and tokenize\n",
    "    doc = nlp(text.lower())\n",
    "    \n",
    "    # loop over the tokens\n",
    "    for token in doc:\n",
    "        \n",
    "        # ignore stopword or punctuation\n",
    "        if(token.text in nlp.Defaults.stop_words or token.text in punctuation):\n",
    "            continue\n",
    "        # append definded POS words\n",
    "        if(token.pos_ in pos_tag):\n",
    "            keyword.append(token.text)\n",
    "    \n",
    "    # to normalize the weightage of the keywords\n",
    "    freq_word = Counter(keyword)\n",
    "    # get the frequency of the top most-common keyword\n",
    "    max_freq = Counter(keyword).most_common(1)[0][1]\n",
    "    # normalize the frequency\n",
    "    for w in freq_word:\n",
    "        freq_word[w] = (freq_word[w]/max_freq)\n",
    "    \n",
    "    # manually increase weight for conclusions and summary\n",
    "    freq_word['conclusion']=10\n",
    "      \n",
    "    \n",
    "    sent_strength = {}\n",
    "    # loop over each sentence\n",
    "    for sent in doc.sents:\n",
    "        \n",
    "        # loop over each word\n",
    "        for word in sent:\n",
    "            # decide if the word is a keyword\n",
    "            if word.text in freq_word.keys():\n",
    "                if sent in sent_strength.keys():\n",
    "                    # add the normalized keyword value to the key-value pair of the sentence\n",
    "                    sent_strength[sent] += freq_word[word.text]\n",
    "                else:\n",
    "                    # create new key-value in the sent_strength dic using sent as key and norm keyword value as value\n",
    "                    sent_strength[sent] = freq_word[word.text]\n",
    "    \n",
    "    summary = []\n",
    "    # sort the dic in descending order\n",
    "    sorted_x = sorted(sent_strength.items(), key = lambda kv: kv[1], reverse=True)\n",
    "    \n",
    "    counter = 0\n",
    "    # loop over each of sorted items\n",
    "    for i in range(len(sorted_x)):\n",
    "        # append result to the list\n",
    "        summary.append(str(sorted_x[i][0]).capitalize())\n",
    "        \n",
    "        counter += 1\n",
    "        if (counter >= limit):\n",
    "            break\n",
    "    \n",
    "    result = ' '.join(summary)\n",
    "    \n",
    "    # fix the case\n",
    "    result2 = fix_case(result, cased)\n",
    "\n",
    "    return result2\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = '''Background: Clostridium ramosum is a generally non-pathogenic enteric anaerobe, and Fournier's gangrene is a rare necrotizing soft tissue infection with male predisposition affecting the perineum and the genital area. We report, to our knowledge, the first case of Fournier's gangrene caused by C. ramosum in a female patient with multiple underlying conditions.\n",
    "\n",
    "Case presentation: A 44-year-old woman with a 6-year history of insulin-dependent diabetes mellitus after total pancreatectomy and an 11-year history of central diabetes insipidus developed a pain in the genital area after a month of urinary catheter use. The lower abdominal pain worsened gradually over 2 weeks, and the pain, general fatigue, and loss of appetite prompted the patient's hospital admission. As she had severe edema in her pelvic and bilateral femoral areas, ceftriaxone was started empirically after collecting two sets of blood cultures. On hospital day 2, CT examination revealed the presence of necrotizing faciitis in the genital and pelvic areas, and the antibiotics were changed to a combination of meropenem, vancomycin, and clindamycin. Gram-positive cocci and gram-positive rods were isolated from blood cultures, which were finally identified as Streptococcus constellatus and C. ramosum using superoxide dismutase and 16S rDNA sequencing. An emergent surgery was performed on hospital day 2 to remove the affected tissue. Despite undergoing debridement and receiving combined antimicrobial chemotherapies, the patient's clinical improvement remained limited. The patient's condition continued to deteriorate, and she eventually died on hospital day 8. In the present case, the underlying diabetes mellitus, urinary incontinence due to central diabetes insipidus, undernutrition, and edema served as the predisposing conditions.\n",
    "\n",
    "Conclusions: C. ramosum is a potentially opportunistic pathogen among immunosuppressed persons and a rare cause of necrotizing fasciitis.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In conclusion, C. ramosum is a potentially opportunistic pathogen among immunosuppressed persons and a rare cause of necrotizing fasciitis. A 44-year-old woman with a 6-year history of insulin-dependent diabetes mellitus after total pancreatectomy and an 11-year history of central diabetes insipidus developed a pain in the genital area after a month of urinary catheter use.\n"
     ]
    }
   ],
   "source": [
    "print(top_sentence(example_text, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
